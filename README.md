# Transformer

Cette implémentation du transformer est basée sur le papier [Attention is All You Need](https://arxiv.org/abs/1706.03762) de Vaswani et al.

Il me sert d'introduction au sujet, afin de comprendre les concepts de base derrière les modèles transformer.

Au fur et à mesure que je progresse, je vais ajouter des notebooks d'expérimentation afin d'explorer chaque composant du transformer et de comprendre leur fonctionnement intuitivement.

## Références

- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Coding a Transformer from Scratch on PyTorch, with full explanation, training and inference](https://www.youtube.com/watch?v=ISNdQcPhsts)
- [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://www.youtube.com/watch?v=bCz4OMemCcA)
- [PyTorch Seq2Seq](https://github.com/bentrevett/pytorch-seq2seq)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)